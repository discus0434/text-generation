{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class AuthenticationInfo:\n",
    "    api_key: str = \"\",\n",
    "    api_secret_key: str = \"\",\n",
    "    bearer_token: str = \"\",\n",
    "    access_token: str = \"\",\n",
    "    access_token_secret: str = \"\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(\n",
    "    src_dir: str = \"sample_texts\", \n",
    "    dst_file: str = \"finetune\", \n",
    "    run_name: str = \"gpt2ja-finetune-small\"\n",
    ") -> None:\n",
    "    \n",
    "    # Encode a dataset\n",
    "    subprocess.run(\n",
    "        f\"python gpt2-japanese/encode_bpe.py --src_dir {src_dir} --dst_file {dst_file}\",\n",
    "        shell=True,\n",
    "    )\n",
    "    \n",
    "    # Fine-Tune\n",
    "    subprocess.run(\n",
    "        f\"python gpt2-japanese/run_finetune.py \\\n",
    "        --base_model gpt2ja-small \\\n",
    "        --dataset {dst_file}.npz \\\n",
    "        --run_name {run_name}\",\n",
    "        shell=True,\n",
    "    )\n",
    "    \n",
    "    # Remove interim files\n",
    "    subprocess.run(f\"rm {dst_file}.npz\", shell=True)\n",
    "    subprocess.run(\"for i in `seq 0 7`; do rm tmp$i.pkl; done\", shell=True)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model: str = \"checkpoints/gpt2ja-finetune-small\",\n",
    "    num_generate: int = 1,\n",
    "    temperature: float = 1.0,\n",
    "    min_length: int = 120,\n",
    "    max_length: int = 140,\n",
    "    output_file: str = \"dist/dist.txt\",\n",
    ") -> str: \n",
    "        \n",
    "    generated = subprocess.run(\n",
    "        f\"TF_CPP_MIN_LOG_LEVEL=3 \\\n",
    "        python gpt2-japanese/gpt2-generate.py \\\n",
    "        --model {model} \\\n",
    "        --num_generate {num_generate} \\\n",
    "        --temperature {temperature} \\\n",
    "        --min_length {min_length} \\\n",
    "        --max_length {max_length} \\\n",
    "        --output_file {output_file}\",\n",
    "        shell=True,\n",
    "        capture_output=True,\n",
    "    )\n",
    "    \n",
    "    return generated.stdout.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweets(\n",
    "    auth_info: AuthenticationInfo\n",
    ") -> None:\n",
    "\n",
    "    client = tweepy.Client(\n",
    "        consumer_key=auth_info.api_key,\n",
    "        consumer_secret=auth_info.api_secret_key,\n",
    "        access_token=auth_info.access_token,\n",
    "        access_token_secret=auth_info.access_token_secret,\n",
    "    )\n",
    "    \n",
    "    today = datetime.strftime(datetime.today(), '%Y-%m-%d')\n",
    "    \n",
    "    # If a fine-tuned GPT-2-ja-small does not exist, make it\n",
    "    if not len(glob.glob(\"./checkpoints/*small\")):\n",
    "\n",
    "        # Remove any other checkpoint if exists\n",
    "        if os.path.exists(\"./checkpoints\"):\n",
    "            subprocess.run(\n",
    "                \"for i in `ls checkpoints | cut -d ' ' -f1`; \\\n",
    "                do rm -rf checkpoints/$i; done\",\n",
    "                shell=True,\n",
    "            )  \n",
    "        \n",
    "        # Do fine-tuning\n",
    "        finetune(\n",
    "            dst_file=f\"{today}-finetune\",\n",
    "            run_name=f\"gpt2ja-{today}-finetune-small\",\n",
    "        )\n",
    "    \n",
    "    model = glob.glob(\"./checkpoints/*small\")[0]\n",
    "        \n",
    "    generated_text = generate(\n",
    "        model=model,\n",
    "        num_generate=1,\n",
    "        output_file=\"dist/dist.txt\",\n",
    "    )\n",
    "    \n",
    "    # print(generated_text)\n",
    "    client.create_tweet(text=generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_info = AuthenticationInfo(\n",
    "    api_key=os.getenv(\"TEINEI_API_KEY\"),\n",
    "    api_secret_key=os.getenv(\"TEINEI_API_SECRET_KEY\"),\n",
    "    access_token=os.getenv(\"TEINEI_ACCESS_TOKEN\"),\n",
    "    access_token_secret=os.getenv(\"TEINEI_ACCESS_TOKEN_SECRET\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS_PER_DAY = 48\n",
    "for i in range(TWEETS_PER_DAY):\n",
    "    create_tweets(auth_info)\n",
    "    time.sleep(86400 // 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('textgen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80cfaf54ddc48834aebaa66aa95b4002d516e0c3f63b9bd458051d5ed5e0fa4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
